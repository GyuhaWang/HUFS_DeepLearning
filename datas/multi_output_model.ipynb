{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/38/d2/3e8c13ffc37ca5ebc6f382b242b44acb43eb489042e1728407ac3904e72f/opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\rui\\.conda\\envs\\hufsg\\lib\\site-packages (from opencv-python) (1.23.5)\n",
      "Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/38.1 MB 2.0 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 3.2/38.1 MB 41.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 8.4/38.1 MB 67.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 13.5/38.1 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 18.6/38.1 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 21.2/38.1 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 23.1/38.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.2/38.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 28.8/38.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.0/38.1 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.1/38.1 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.1/38.1 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.1/38.1 MB 65.5 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-WM7nvf41Xz",
    "outputId": "243fd2d1-0b49-44e1-b16d-37c1e59f0743"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "#from google.colab import drive\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "#!drive unmount\n",
    "#!drive mount '/content/drive', force_remount=True\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TSjrCptXu5Lf"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_file = zipfile.ZipFile('/content/drive/MyDrive/data.zip') # 압축을 해제할 '/파일경로/파일명.zip'\n",
    "zip_file.extractall('/content/') # 압축을 해제할 '/위치경로/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2dxptZ165k04"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 이미지 zip 파일 경로\n",
    "validation_folder_path = 'data/Validation'\n",
    "training_folder_path = 'data/Training'\n",
    "\n",
    "training_image_folder_path = os.listdir(os.path.join(training_folder_path,'data'))\n",
    "training_label_folder_path = os.listdir(os.path.join(training_folder_path,'label'))\n",
    "#ex) Ts..1960m Ts..1970...\n",
    "validation_image_folder_path = os.listdir(os.path.join(validation_folder_path,'data'))\n",
    "validation_label_folder_path = os.listdir(os.path.join(validation_folder_path,'label'))\n",
    "# training_image_folders =os.listdir(training_image_folder_path)\n",
    "# training_label_folders =os.listdir(training_label_folder_path)\n",
    "# validation_image_folders =os.listdir(validation_image_folder_path)\n",
    "# validation_label_folders =os.listdir(validation_label_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TL_man_1950', 'TL_man_1960', 'TL_man_1970', 'TL_man_1980', 'TL_man_1990', 'TL_man_2000', 'TL_man_2010', 'TL_man_2019', 'TL_woman_1950', 'TL_woman_1960', 'TL_woman_1970', 'TL_woman_1980', 'TL_woman_1990', 'TL_woman_2000', 'TL_woman_2010', 'TL_woman_2019']\n",
      "['TS_man_1950', 'TS_man_1960', 'TS_man_1970', 'TS_man_1980', 'TS_man_1990', 'TS_man_2000', 'TS_man_2010', 'TS_man_2019', 'TS_woman_1950', 'TS_woman_1960', 'TS_woman_1970', 'TS_woman_1980', 'TS_woman_1990', 'TS_woman_2000', 'TS_woman_2010', 'TS_woman_2019']\n"
     ]
    }
   ],
   "source": [
    "print(training_label_folder_path)\n",
    "print(training_image_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_group(data,category):\n",
    "    label_dict = {data: i for i, data in enumerate(category)}\n",
    "    #print(label_dict)\n",
    "    data_indices = [label_dict[str(data)]]\n",
    "    #print(label_dict)\n",
    "    one_hot = torch.nn.functional.one_hot(torch.tensor(data_indices), len(category))\n",
    "    #print(one_hot)\n",
    "    return one_hot\n",
    "    \n",
    "def combine_one_hot_labels(*vectors):\n",
    "    return torch.cat(vectors, dim=0)\n",
    "\n",
    "def add_extension(img_path):\n",
    "    if not img_path.endswith(\".jpg\"):\n",
    "        img_path += \".jpg\"\n",
    "    return img_path\n",
    "\n",
    "def one_hot_encode_label(folder_path,file_name,label_name):\n",
    "    # 라벨 파일 읽기\n",
    "    file_path = folder_path+\"/\"+file_name\n",
    "    with open(file_path, 'r') as file:\n",
    "        label_data = json.load(file)\n",
    "\n",
    "    if label_name == \"image\":\n",
    "        img_path = label_data[\"imgName\"]\n",
    "        img_path_with_extension = add_extension(img_path)\n",
    "        return img_path_with_extension\n",
    "    \n",
    "    era = str(label_data[\"era\"])\n",
    "    era_category = ['1950','1960','1970','1980','1990','2000','2010','2019']\n",
    "    \n",
    "    style = label_data[\"style\"]\n",
    "    style_category = [\"ivy\",\"feminine\",\"classic\",\"mods\",\"minimal\",\"popart\",\"space\",\"hippie\",\"disco\",\"military\",\"punk\",\"bold\",\"powersuit\",\"bodyconscious\",\"hiphop\",\"kitsch\",\"lingerie\",\"grunge\",\"metrosexual\",\"cityglam\",\"oriental\",\"ecology\",\"sportivecasual\",\"athleisure\",\"lounge\",\"normcore\",\"genderless\"]\n",
    "    \n",
    "    gender = label_data[\"gender\"]\n",
    "    gender_category = ['M','W']\n",
    "    \n",
    "    survey_data = label_data[\"survey\"]\n",
    "    q1 = survey_data[\"Q1\"]\n",
    "    q1_category = [\"1\",\"2\",\"3\",\"4\"]\n",
    "    q2 = survey_data[\"Q2\"] # 데이터가 여러개임\n",
    "    q2_category = [\"1\",\"2\",\"3\"]\n",
    "    q3 = survey_data[\"Q3\"] # 데이터가 여러개임\n",
    "    q3_category = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"]\n",
    "    q411 = survey_data[\"Q411\"]\n",
    "    q411_category = [\"1\",\"2\",\"3\"]\n",
    "    q412 = survey_data[\"Q412\"]\n",
    "    q412_category = [\"1\",\"2\"]\n",
    "    q413 = survey_data[\"Q413\"]\n",
    "    q413_category = [\"1\",\"2\"]\n",
    "    q414 = survey_data[\"Q414\"]\n",
    "    q414_category = [\"1\",\"2\"]\n",
    "    q42xx_category = [\"0\",\"1\"]\n",
    "    q5 = survey_data[\"Q5\"]\n",
    "    q5_category = [\"1\",\"2\"]\n",
    "\n",
    "    one_hot_result = \"\"\n",
    "\n",
    "    #'''\n",
    "    if label_name == \"era\":\n",
    "        one_hot_result = one_hot_group(era, era_category)\n",
    "    elif label_name == \"style\":\n",
    "        one_hot_result = one_hot_group(style, style_category)\n",
    "    elif label_name == \"gender\":\n",
    "        one_hot_result = one_hot_group(gender, gender_category)\n",
    "    elif label_name == \"q1\":\n",
    "        one_hot_result = one_hot_group(q1,q1_category)\n",
    "    elif label_name == \"q2\":\n",
    "        one_hot_result = one_hot_group(q2,q2_category)\n",
    "    elif label_name == \"q3\":\n",
    "        one_hot_result = one_hot_group(q3,q3_category)\n",
    "    elif label_name == \"q411\":\n",
    "        one_hot_result = one_hot_group(q411,q411_category)\n",
    "    elif label_name == \"q412\":\n",
    "        one_hot_result = one_hot_group(q412,q412_category)\n",
    "    elif label_name == \"q413\":\n",
    "        one_hot_result = one_hot_group(q413,q413_category)\n",
    "    elif label_name == \"q414\":\n",
    "        one_hot_result = one_hot_group(q414,q414_category)\n",
    "    #'''\n",
    "    for i in range(0, 16):\n",
    "        key = f\"Q{4201+i}\"\n",
    "        if label_name == f\"q{4201+i}\":\n",
    "            one_hot_result = one_hot_group(survey_data[key],q42xx_category)\n",
    "            break\n",
    "    '''\n",
    "    combined_label = combine_one_hot_vectors(\n",
    "    era_one_hot, style_one_hot, gender_one_hot,\n",
    "    q1_one_hot, \n",
    "        #q2_one_hot, q3_one_hot,\n",
    "    q411_one_hot, q412_one_hot, q413_one_hot, q414_one_hot, *q42xx_one_hot\n",
    "    )\n",
    "    print(combined_label)\n",
    "    print(combined_label.shape)\n",
    "    '''\n",
    "    \n",
    "    return one_hot_result\n",
    "\n",
    "def label_one_hot(image_path,label_path,label_name):\n",
    "    label_list = os.listdir(label_path)\n",
    "    temp = []\n",
    "    for file_name in label_list:\n",
    "        image_name = one_hot_encode_label(label_path,file_name,\"image\")\n",
    "        label = []\n",
    "        for labels in label_name:\n",
    "            label.extend(one_hot_encode_label(label_path,file_name,labels))\n",
    "        image = image_path +'/'+image_name\n",
    "        data = (image,label)\n",
    "        temp.append(data)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Training image 폴더 안에 있는 모든 서브폴더명:\", subfolder_names_training_image)\\nprint(\"Training label 폴더 안에 있는 모든 서브폴더명:\", subfolder_names_training_label)\\nprint(\"Vaildation image 폴더 안에 있는 모든 서브폴더명:\", subfolder_names_validation_image)\\nprint(\"Vaildation label 폴더 안에 있는 모든 서브폴더명:\", subfolder_names_validation_label)\\n#'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_subfolder_names(folder_path):\n",
    "    subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "    return subfolders\n",
    "\n",
    "#'''\n",
    "folder_path_training_image = 'data/Training/data'\n",
    "folder_path_validation_image = 'data/Validation/data'\n",
    "folder_path_training_label = 'data/Training/label'\n",
    "folder_path_validation_label = 'data/Validation/label'\n",
    "subfolder_names_training_image = get_subfolder_names(folder_path_training_image)\n",
    "subfolder_names_validation_image = get_subfolder_names(folder_path_validation_image)\n",
    "subfolder_names_training_label = get_subfolder_names(folder_path_training_label)\n",
    "subfolder_names_validation_label = get_subfolder_names(folder_path_validation_label)\n",
    "#'''\n",
    "'''\n",
    "print(\"Training image 폴더 안에 있는 모든 서브폴더명:\", subfolder_names_training_image)\n",
    "print(\"Training label 폴더 안에 있는 모든 서브폴더명:\", subfolder_names_training_label)\n",
    "print(\"Vaildation image 폴더 안에 있는 모든 서브폴더명:\", subfolder_names_validation_image)\n",
    "print(\"Vaildation label 폴더 안에 있는 모든 서브폴더명:\", subfolder_names_validation_label)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data 'TS_man_1950' is OK\n",
      "Training Data 'TS_man_1960' is OK\n",
      "Training Data 'TS_man_1970' is OK\n",
      "Training Data 'TS_man_1980' is OK\n",
      "Training Data 'TS_man_1990' is OK\n",
      "Training Data 'TS_man_2000' is OK\n",
      "Training Data 'TS_man_2010' is OK\n",
      "Training Data 'TS_man_2019' is OK\n",
      "Training Data 'TS_woman_1950' is OK\n",
      "Training Data 'TS_woman_1960' is OK\n",
      "Training Data 'TS_woman_1970' is OK\n",
      "Training Data 'TS_woman_1980' is OK\n",
      "Training Data 'TS_woman_1990' is OK\n",
      "Training Data 'TS_woman_2000' is OK\n",
      "Training Data 'TS_woman_2010' is OK\n",
      "Training Data 'TS_woman_2019' is OK\n",
      "Validation Data 'VS_man_1950' is OK\n",
      "Validation Data 'VS_man_1960' is OK\n",
      "Validation Data 'VS_man_1970' is OK\n",
      "Validation Data 'VS_man_1980' is OK\n",
      "Validation Data 'VS_man_1990' is OK\n",
      "Validation Data 'VS_man_2000' is OK\n",
      "Validation Data 'VS_man_2010' is OK\n",
      "Validation Data 'VS_man_2019' is OK\n",
      "Validation Data 'VS_woman_1950' is OK\n",
      "Validation Data 'VS_woman_1960' is OK\n",
      "Validation Data 'VS_woman_1970' is OK\n",
      "Validation Data 'VS_woman_1980' is OK\n",
      "Validation Data 'VS_woman_1990' is OK\n",
      "Validation Data 'VS_woman_2000' is OK\n",
      "Validation Data 'VS_woman_2010' is OK\n",
      "Validation Data 'VS_woman_2019' is OK\n"
     ]
    }
   ],
   "source": [
    "label_list = [\"era\",\"style\",\"gender\",\"q1\",\"q2\",\"q3\",\"q411\",\"q412\",\"q413\",\"q414\",\"q4201\",\"q4202\",\"q4203\",\"q4204\",\"q4205\",\"q4206\",\"q4207\",\"q4208\",\"q4209\",\"q4210\",\"q4211\",\"q4212\",\"q4213\",\"q4214\",\"q4215\",\"q4216\",\"q5\"]\n",
    "train_data = []\n",
    "validation_data =[]\n",
    "#label_list = [\"style\",\"gender\",\"q1\"]\n",
    "#'''\n",
    "for image_path, label_path in zip(subfolder_names_training_image,subfolder_names_training_label):\n",
    "    train_data.extend(label_one_hot(folder_path_training_image + '/' + image_path , folder_path_training_label +'/'+ label_path,label_list))\n",
    "    print(f\"Training Data '{image_path}' is OK\")\n",
    "    #print(folder_path_training_image + '/' + image_path , folder_path_training_label +'/'+ label_path)\n",
    "#'''\n",
    "#'''\n",
    "for image_path, label_path in zip(subfolder_names_validation_image,subfolder_names_validation_label):\n",
    "    validation_data.extend(label_one_hot(folder_path_validation_image + '/' + image_path , folder_path_validation_label +'/'+ label_path,label_list))\n",
    "    print(f\"Validation Data '{image_path}' is OK\")\n",
    "    #print(folder_path_vaildation+'/'+ label_path)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data/Training/data/TS_man_1950/T_00001_50_ivy_M.jpg', [tensor([1, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]), tensor([1, 0]), tensor([0, 0, 1, 0]), tensor([1, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0]), tensor([0, 1, 0]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([1, 0]), tensor([1, 0]), tensor([1, 0]), tensor([1, 0]), tensor([0, 1]), tensor([1, 0]), tensor([1, 0]), tensor([0, 1]), tensor([1, 0]), tensor([1, 0]), tensor([0, 1]), tensor([1, 0]), tensor([1, 0]), tensor([1, 0]), tensor([1, 0]), tensor([1, 0])])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path error check\n",
    "for index, (image_path, _) in enumerate(train_data):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error at index {index}: File not found - {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2개\n",
    "class facedataset(Dataset):\n",
    "    def __init__(self, data, train=True, test=False, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        if test:\n",
    "            self.images = [img for img, labels in data]\n",
    "            self.labels = [labels for img, labels in data]\n",
    "            return\n",
    "\n",
    "        num_samples = len(data)\n",
    "        num_test_samples = int(0.2 * num_samples)\n",
    "        num_train_samples = num_samples - num_test_samples\n",
    "\n",
    "        train_files, test_files = torch.utils.data.random_split(data, [num_train_samples, num_test_samples])\n",
    "\n",
    "        if train:\n",
    "            self.images = [img for img, labels in train_files]\n",
    "            self.labels = [labels for img, labels in train_files]\n",
    "        else:\n",
    "            self.images = [img for img, labels in test_files]\n",
    "            self.labels = [labels for img, labels in test_files]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        labels = self.labels[idx]\n",
    "        return img, *labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "yB-Xh6fHphIj"
   },
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Resize(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    ])\n",
    "\n",
    "train_dataset = facedataset(train_data, train= True, transform= transform)\n",
    "validation_dataset =facedataset(train_data, train= False, transform= transform)\n",
    "test_dataset  = facedataset(validation_data, test= True,transform= transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_data_loader = DataLoader(validation_dataset,batch_size=batch_size,)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2131\n",
      "533\n",
      "417\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_loader))\n",
    "print(len(validation_data_loader))\n",
    "print(len(test_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 27, 2, 4, 3, 8, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "label_classes = []\n",
    "data_ex = next(iter(validation_dataset))\n",
    "for i in range(1,len(data_ex)):\n",
    "    labelSize = data_ex[i].size()\n",
    "    label_classes.append(labelSize.numel())\n",
    "print(label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\rui\\.conda\\envs\\hufsg\\lib\\site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCnzSDW87Mrv",
    "outputId": "3a05c78c-6ded-48ec-9df8-8511b3f883a1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nloss_list = []\\naccuracy_list = []\\n\\nfor epoch in range(num_epochs):\\n    # 8\\n    cost=0\\n    model.train()\\n    for images, labels in train_data_loader:\\n        # 9\\n        images = images.to(device)\\n        labels = labels.to(device)\\n\\n        # 10\\n        outputs = model(images)\\n        loss = criterion(outputs, labels)\\n\\n        # 11\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        cost += loss\\n\\n    # # # 12\\n    model.eval()\\n    test_loss = 0.0\\n    correct = 0\\n\\n    # # 13\\n    with torch.no_grad():\\n        for images, labels in validation_data_loader:\\n            images = images.to(device)\\n            labels = labels.to(device)\\n\\n            # 14\\n            outputs = model(images)\\n            predicted = torch.max(outputs, 1)[1]\\n            loss = criterion(outputs, labels)\\n\\n            # 15\\n            test_loss += loss.item()\\n            ans =torch.max(labels, 1)[1]\\n\\n            correct += (ans == predicted).sum()\\n    # 16\\n    avg_cost = cost / len(train_data_loader)\\n    accuracy = correct/len(validation_data_loader)\\n\\n\\n\\n    print(\"epoch : {} | loss : {:.6f}\" .format(epoch, avg_cost))\\n    print(\"Accuracy : {:.2f}\".format(accuracy))\\n    print(\"------\")\\n\\n    # plt.figure(figsize=(10,5))\\n    # plt.subplot(1,2,1)\\n    # plt.xlabel(\\'Epoch\\')\\n    # plt.ylabel(\\'Loss\\')\\n    # plt.plot(epoch,loss_list)\\n    # plt.subplot(1,2,2)\\n    # plt.xlabel(\\'Epoch\\')\\n    # plt.ylabel(\\'Accuracy\\')\\n    # plt.plot(epoch, accuracy_list)\\n    # plt.show()\\n#'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1개만 들어갈때\n",
    "'''\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 8\n",
    "    cost=0\n",
    "    model.train()\n",
    "    for images, labels in train_data_loader:\n",
    "        # 9\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 10\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 11\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cost += loss\n",
    "\n",
    "    # # # 12\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    # # 13\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 14\n",
    "            outputs = model(images)\n",
    "            predicted = torch.max(outputs, 1)[1]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 15\n",
    "            test_loss += loss.item()\n",
    "            ans =torch.max(labels, 1)[1]\n",
    "\n",
    "            correct += (ans == predicted).sum()\n",
    "    # 16\n",
    "    avg_cost = cost / len(train_data_loader)\n",
    "    accuracy = correct/len(validation_data_loader)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"epoch : {} | loss : {:.6f}\" .format(epoch, avg_cost))\n",
    "    print(\"Accuracy : {:.2f}\".format(accuracy))\n",
    "    print(\"------\")\n",
    "\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # plt.subplot(1,2,1)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.plot(epoch,loss_list)\n",
    "    # plt.subplot(1,2,2)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Accuracy')\n",
    "    # plt.plot(epoch, accuracy_list)\n",
    "    # plt.show()\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, label_classes):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        \n",
    "        # 기존의 VGG16 모델 불러오기\n",
    "        vgg16_model = models.vgg16(pretrained=True)\n",
    "        for param in vgg16_model.parameters():\n",
    "            param.requires_grad = False;\n",
    "            \n",
    "        self.features = vgg16_model.features\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "            \n",
    "        # Create classifiers dynamically based on the length of label_classes\n",
    "        classifiers = []\n",
    "        for label_class in label_classes:\n",
    "            classifiers.append(nn.Sequential(\n",
    "                nn.Linear(25088, label_class),\n",
    "            ))\n",
    "\n",
    "        # Use ModuleList to register dynamically created modules\n",
    "        self.classifiers = nn.ModuleList(classifiers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        outputs = [classifier(x) for classifier in self.classifiers]\n",
    "\n",
    "        return tuple(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 27, 2, 4, 3, 8, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Using device: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "          Flatten-32                [-1, 25088]               0\n",
      "           Linear-33                    [-1, 8]         200,712\n",
      "           Linear-34                   [-1, 27]         677,403\n",
      "           Linear-35                    [-1, 2]          50,178\n",
      "           Linear-36                    [-1, 4]         100,356\n",
      "           Linear-37                    [-1, 3]          75,267\n",
      "           Linear-38                    [-1, 8]         200,712\n",
      "           Linear-39                    [-1, 3]          75,267\n",
      "           Linear-40                    [-1, 2]          50,178\n",
      "           Linear-41                    [-1, 2]          50,178\n",
      "           Linear-42                    [-1, 2]          50,178\n",
      "           Linear-43                    [-1, 2]          50,178\n",
      "           Linear-44                    [-1, 2]          50,178\n",
      "           Linear-45                    [-1, 2]          50,178\n",
      "           Linear-46                    [-1, 2]          50,178\n",
      "           Linear-47                    [-1, 2]          50,178\n",
      "           Linear-48                    [-1, 2]          50,178\n",
      "           Linear-49                    [-1, 2]          50,178\n",
      "           Linear-50                    [-1, 2]          50,178\n",
      "           Linear-51                    [-1, 2]          50,178\n",
      "           Linear-52                    [-1, 2]          50,178\n",
      "           Linear-53                    [-1, 2]          50,178\n",
      "           Linear-54                    [-1, 2]          50,178\n",
      "           Linear-55                    [-1, 2]          50,178\n",
      "           Linear-56                    [-1, 2]          50,178\n",
      "           Linear-57                    [-1, 2]          50,178\n",
      "           Linear-58                    [-1, 2]          50,178\n",
      "================================================================\n",
      "Total params: 17,047,965\n",
      "Trainable params: 2,333,277\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.59\n",
      "Params size (MB): 65.03\n",
      "Estimated Total Size (MB): 284.19\n",
      "----------------------------------------------------------------\n",
      "MultiOutputModel(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (classifiers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=8, bias=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=27, bias=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=2, bias=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4, bias=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=3, bias=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=8, bias=True)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=3, bias=True)\n",
      "    )\n",
      "    (7-25): 19 x Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "label_classes = []\n",
    "data_ex = next(iter(validation_dataset))\n",
    "for i in range(1,len(data_ex)):\n",
    "    labelSize = data_ex[i].size()\n",
    "    label_classes.append(labelSize.numel())\n",
    "print(label_classes)\n",
    "\n",
    "model = MultiOutputModel(label_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Using device:\", device)\n",
    "'''\n",
    "update_param_names = []\n",
    "for classifier in [model.classifier1, model.classifier2, model.classifier3]:\n",
    "    for layer in classifier.children():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.to(device)\n",
    "            update_param_names.extend([layer.weight, layer.bias])\n",
    "'''\n",
    "summary(model, (3, 224, 224))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "M614Xmznpzoo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.1820,  0.0260, -0.1748,  ..., -0.3554, -0.5805, -0.0636],\n",
      "        [ 0.0032, -0.0253,  0.0504,  ..., -0.0445, -0.1234,  0.0558],\n",
      "        [ 0.0681,  0.3025,  0.0410,  ...,  0.1815,  0.2008,  0.1551],\n",
      "        ...,\n",
      "        [ 0.0044,  0.1339,  0.1304,  ...,  0.0665,  0.1567,  0.0853],\n",
      "        [-0.0616, -0.1244, -0.0192,  ..., -0.0467,  0.2553,  0.0821],\n",
      "        [ 0.0674, -0.3210,  0.2214,  ..., -0.2548,  0.0677, -0.2591]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0376,  0.0282, -0.0228,  0.0361, -0.0711,  0.0540, -0.1283,  0.1389],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0567,  0.0718,  0.0988,  ..., -0.1459, -0.0331,  0.0342],\n",
      "        [-0.0165, -0.0564, -0.0565,  ...,  0.0320, -0.3942, -0.0737],\n",
      "        [ 0.1018, -0.0524, -0.1645,  ..., -0.2760, -0.2049,  0.0172],\n",
      "        ...,\n",
      "        [-0.0158,  0.2143,  0.2053,  ..., -0.1169,  0.0660,  0.0033],\n",
      "        [-0.2158, -0.1956, -0.2725,  ...,  0.0160,  0.0701, -0.1079],\n",
      "        [ 0.2690, -0.0280,  0.1523,  ..., -0.1301, -0.0281,  0.0561]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 3.0874e-02, -5.1294e-02,  1.5044e-02,  6.8995e-03,  3.7310e-02,\n",
      "        -1.3948e-02, -3.9154e-02, -3.5247e-02,  9.3529e-03, -1.2124e-02,\n",
      "        -9.3856e-03,  6.6631e-05,  4.0483e-02,  2.5910e-02, -4.6563e-02,\n",
      "        -3.8562e-02, -2.8062e-03,  1.6617e-02, -2.4124e-02, -5.4031e-03,\n",
      "        -1.7339e-02,  4.3517e-02, -1.1620e-01,  2.6691e-03,  4.5970e-02,\n",
      "         1.2046e-01,  2.0120e-02], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.4363, -0.0352, -0.3056,  ...,  0.2706,  0.1473, -0.3068],\n",
      "        [ 0.4306,  0.0428,  0.3054,  ..., -0.2737, -0.1505,  0.3047]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0698,  0.0698], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0469,  0.0405, -0.0891,  ...,  0.2071,  0.0731,  0.1636],\n",
      "        [ 0.0091,  0.0795,  0.0351,  ...,  0.1265,  0.2410,  0.0412],\n",
      "        [-0.0648, -0.0461, -0.1681,  ..., -0.1249, -0.1785, -0.2464],\n",
      "        [ 0.1147, -0.0809,  0.2323,  ..., -0.2110, -0.1329,  0.0489]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2184,  0.1368,  0.0550,  0.0233], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2146,  0.2367,  0.2192,  ...,  0.2155, -0.3107,  0.2664],\n",
      "        [ 0.0369, -0.1957, -0.0394,  ...,  0.1196,  0.2765, -0.2360],\n",
      "        [ 0.1828, -0.0419, -0.1770,  ..., -0.3284,  0.0296, -0.0222]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0676, -0.1106,  0.0467], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0834, -0.0557, -0.0794,  ..., -0.1389, -0.1060,  0.0109],\n",
      "        [-0.3187, -0.1539,  0.0480,  ..., -0.1403,  0.1481,  0.1157],\n",
      "        [ 0.0733,  0.2263,  0.0460,  ...,  0.1008,  0.0295,  0.0666],\n",
      "        ...,\n",
      "        [ 0.0589,  0.1762,  0.2060,  ...,  0.0114,  0.2712,  0.0665],\n",
      "        [-0.0275, -0.0863,  0.1666,  ...,  0.2294,  0.0512, -0.0209],\n",
      "        [-0.0840,  0.0928, -0.2042,  ...,  0.6327,  0.1862, -0.3021]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0122, -0.0316, -0.0296,  0.0076,  0.1254, -0.0810, -0.0119,  0.0199],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0166, -0.0634,  0.0551,  ...,  0.4698,  0.0112,  0.2595],\n",
      "        [-0.4524, -0.3431, -0.0869,  ..., -0.5369, -0.2506, -0.1517],\n",
      "        [ 0.4351,  0.4073,  0.0319,  ...,  0.0657,  0.2413, -0.1111]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0472,  0.1866, -0.1333], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0732, -0.1945,  0.0633,  ...,  0.3042, -0.0291, -0.2385],\n",
      "        [ 0.0759,  0.1897, -0.0599,  ..., -0.3062,  0.0317,  0.2342]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1140, -0.1169], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.4001, -0.0647,  0.0786,  ...,  0.0620, -0.1487, -0.1472],\n",
      "        [-0.3944,  0.0616, -0.0859,  ..., -0.0649,  0.1446,  0.1505]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2301,  0.2287], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0140, -0.0315,  0.0749,  ...,  0.3210,  0.2794,  0.2560],\n",
      "        [-0.0175,  0.0342, -0.0734,  ..., -0.3225, -0.2809, -0.2519]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0571, -0.0591], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0293,  0.0204,  0.0710,  ..., -0.1110, -0.1334,  0.0655],\n",
      "        [ 0.0303, -0.0229, -0.0760,  ...,  0.1090,  0.1372, -0.0713]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0729, -0.0745], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1710,  0.0714, -0.0508,  ...,  0.5673,  0.1276, -0.0839],\n",
      "        [-0.1649, -0.0713,  0.0463,  ..., -0.5670, -0.1224,  0.0884]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0570, -0.0543], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2030,  0.0719, -0.2217,  ..., -0.2036,  0.4000, -0.2079],\n",
      "        [-0.2001, -0.0755,  0.2164,  ...,  0.2053, -0.3974,  0.2051]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1528, -0.1533], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0446,  0.1592,  0.0736,  ...,  0.1072,  0.0775,  0.0151],\n",
      "        [-0.0423, -0.1612, -0.0720,  ..., -0.1025, -0.0782, -0.0143]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1495, -0.1443], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2408, -0.0270,  0.1596,  ...,  0.5772,  0.1040, -0.1198],\n",
      "        [ 0.2387,  0.0334, -0.1627,  ..., -0.5753, -0.1096,  0.1121]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0159, -0.0181], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1587,  0.1829,  0.0119,  ...,  0.0518,  0.0221,  0.0497],\n",
      "        [-0.1635, -0.1846, -0.0187,  ..., -0.0523, -0.0160, -0.0539]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1685, -0.1655], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2360,  0.1657,  0.0541,  ..., -0.1699, -0.8516, -0.1979],\n",
      "        [-0.2316, -0.1650, -0.0541,  ...,  0.1713,  0.8532,  0.2026]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2007, -0.2020], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1378, -0.5562, -0.1736,  ...,  0.3648,  0.4143, -0.0775],\n",
      "        [ 0.1424,  0.5567,  0.1713,  ..., -0.3658, -0.4153,  0.0775]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0039,  0.0046], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0313,  0.0278,  0.1627,  ...,  0.2062, -0.0820,  0.0563],\n",
      "        [ 0.0322, -0.0220, -0.1603,  ..., -0.2011,  0.0863, -0.0545]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2154, -0.2142], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1527, -0.2184,  0.2986,  ...,  0.6371,  0.3838,  0.1532],\n",
      "        [ 0.1578,  0.2215, -0.3021,  ..., -0.6349, -0.3893, -0.1506]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0099,  0.0135], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2077,  0.0510, -0.0904,  ...,  0.4870, -0.2447, -0.1048],\n",
      "        [-0.2067, -0.0501,  0.0839,  ..., -0.4865,  0.2500,  0.1051]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1143, -0.1171], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2044, -0.2016, -0.3131,  ...,  0.1557, -0.0717, -0.0278],\n",
      "        [ 0.2095,  0.1999,  0.3132,  ..., -0.1595,  0.0802,  0.0211]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0226,  0.0239], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0249, -0.1195, -0.1165,  ...,  0.1480, -0.4877, -0.1860],\n",
      "        [ 0.0267,  0.1195,  0.1150,  ..., -0.1492,  0.4896,  0.1844]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1271, -0.1271], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0019, -0.2056, -0.2378,  ..., -0.1406,  0.1975,  0.2084],\n",
      "        [-0.0094,  0.2116,  0.2381,  ...,  0.1357, -0.1957, -0.2107]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0119,  0.0090], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1039,  0.1099,  0.2588,  ...,  0.1346,  0.1517,  0.1764],\n",
      "        [ 0.0984, -0.1144, -0.2663,  ..., -0.1351, -0.1505, -0.1797]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0279, -0.0243], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1090,  0.0800, -0.1163,  ...,  0.0728, -0.0682,  0.0007],\n",
      "        [-0.1037, -0.0805,  0.1096,  ..., -0.0767,  0.0720, -0.0044]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0014,  0.0043], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(params)\n",
    "optimizer = torch.optim.SGD(params, lr=0.001,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.0004, Validation Loss: 1.8412\n",
      "Training Accuracy : \n",
      "Accuracy1: 71.78%, Accuracy2: 86.79%, Accuracy3: 94.09%, Accuracy4: 67.04%, Accuracy5: 81.92%, Accuracy6: 67.67%, Accuracy7: 81.11%, Accuracy8: 87.85%, Accuracy9: 77.98%, Accuracy10: 84.12%, Accuracy11: 92.33%, Accuracy12: 84.08%, Accuracy13: 82.60%, Accuracy14: 84.95%, Accuracy15: 85.86%, Accuracy16: 90.14%, Accuracy17: 82.76%, Accuracy18: 88.25%, Accuracy19: 91.80%, Accuracy20: 89.65%, Accuracy21: 85.84%, Accuracy22: 85.98%, Accuracy23: 91.42%, Accuracy24: 90.65%, Accuracy25: 96.80%, Accuracy26: 96.51%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 66.87%, Accuracy2: 76.66%, Accuracy3: 92.83%, Accuracy4: 58.23%, Accuracy5: 80.07%, Accuracy6: 58.30%, Accuracy7: 79.69%, Accuracy8: 84.04%, Accuracy9: 75.91%, Accuracy10: 83.71%, Accuracy11: 91.90%, Accuracy12: 84.00%, Accuracy13: 79.74%, Accuracy14: 85.59%, Accuracy15: 82.81%, Accuracy16: 89.31%, Accuracy17: 82.88%, Accuracy18: 87.01%, Accuracy19: 90.71%, Accuracy20: 88.47%, Accuracy21: 84.10%, Accuracy22: 76.42%, Accuracy23: 90.45%, Accuracy24: 87.21%, Accuracy25: 95.64%, Accuracy26: 95.77%, \n",
      "Epoch [2/10], Training Loss: 0.0006, Validation Loss: 1.7298\n",
      "Training Accuracy : \n",
      "Accuracy1: 72.71%, Accuracy2: 87.83%, Accuracy3: 94.24%, Accuracy4: 67.35%, Accuracy5: 82.18%, Accuracy6: 67.98%, Accuracy7: 81.25%, Accuracy8: 88.03%, Accuracy9: 78.14%, Accuracy10: 83.97%, Accuracy11: 92.53%, Accuracy12: 84.16%, Accuracy13: 82.90%, Accuracy14: 84.95%, Accuracy15: 86.03%, Accuracy16: 90.15%, Accuracy17: 82.68%, Accuracy18: 88.14%, Accuracy19: 91.84%, Accuracy20: 89.87%, Accuracy21: 85.81%, Accuracy22: 85.94%, Accuracy23: 91.56%, Accuracy24: 90.90%, Accuracy25: 96.77%, Accuracy26: 96.62%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 66.73%, Accuracy2: 77.91%, Accuracy3: 90.58%, Accuracy4: 61.24%, Accuracy5: 80.96%, Accuracy6: 63.44%, Accuracy7: 79.66%, Accuracy8: 86.27%, Accuracy9: 72.83%, Accuracy10: 83.04%, Accuracy11: 92.71%, Accuracy12: 84.99%, Accuracy13: 83.47%, Accuracy14: 85.80%, Accuracy15: 86.63%, Accuracy16: 87.67%, Accuracy17: 79.88%, Accuracy18: 87.42%, Accuracy19: 91.93%, Accuracy20: 83.60%, Accuracy21: 85.11%, Accuracy22: 83.63%, Accuracy23: 86.05%, Accuracy24: 90.38%, Accuracy25: 94.35%, Accuracy26: 96.00%, \n",
      "Epoch [3/10], Training Loss: 0.0006, Validation Loss: 1.6754\n",
      "Training Accuracy : \n",
      "Accuracy1: 73.12%, Accuracy2: 88.90%, Accuracy3: 94.32%, Accuracy4: 67.06%, Accuracy5: 82.26%, Accuracy6: 68.41%, Accuracy7: 81.32%, Accuracy8: 87.83%, Accuracy9: 77.91%, Accuracy10: 84.16%, Accuracy11: 92.71%, Accuracy12: 84.30%, Accuracy13: 82.99%, Accuracy14: 84.99%, Accuracy15: 86.15%, Accuracy16: 90.26%, Accuracy17: 82.86%, Accuracy18: 88.37%, Accuracy19: 92.11%, Accuracy20: 89.90%, Accuracy21: 85.74%, Accuracy22: 86.10%, Accuracy23: 91.55%, Accuracy24: 91.12%, Accuracy25: 96.93%, Accuracy26: 96.75%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 67.77%, Accuracy2: 78.22%, Accuracy3: 93.10%, Accuracy4: 65.17%, Accuracy5: 79.95%, Accuracy6: 64.46%, Accuracy7: 78.95%, Accuracy8: 86.78%, Accuracy9: 77.45%, Accuracy10: 80.18%, Accuracy11: 90.85%, Accuracy12: 84.84%, Accuracy13: 83.53%, Accuracy14: 85.57%, Accuracy15: 85.80%, Accuracy16: 89.45%, Accuracy17: 79.10%, Accuracy18: 83.51%, Accuracy19: 91.61%, Accuracy20: 88.63%, Accuracy21: 85.21%, Accuracy22: 85.27%, Accuracy23: 89.25%, Accuracy24: 89.00%, Accuracy25: 96.00%, Accuracy26: 95.93%, \n",
      "Epoch [4/10], Training Loss: 0.0005, Validation Loss: 1.6032\n",
      "Training Accuracy : \n",
      "Accuracy1: 73.77%, Accuracy2: 89.54%, Accuracy3: 94.37%, Accuracy4: 67.49%, Accuracy5: 82.43%, Accuracy6: 69.01%, Accuracy7: 81.37%, Accuracy8: 88.27%, Accuracy9: 77.78%, Accuracy10: 84.42%, Accuracy11: 92.70%, Accuracy12: 84.26%, Accuracy13: 82.90%, Accuracy14: 85.11%, Accuracy15: 86.35%, Accuracy16: 90.42%, Accuracy17: 82.88%, Accuracy18: 88.45%, Accuracy19: 92.18%, Accuracy20: 90.13%, Accuracy21: 85.76%, Accuracy22: 86.12%, Accuracy23: 91.66%, Accuracy24: 91.01%, Accuracy25: 97.05%, Accuracy26: 96.77%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 67.68%, Accuracy2: 78.82%, Accuracy3: 93.64%, Accuracy4: 64.23%, Accuracy5: 80.42%, Accuracy6: 63.64%, Accuracy7: 80.01%, Accuracy8: 86.06%, Accuracy9: 75.66%, Accuracy10: 83.76%, Accuracy11: 92.28%, Accuracy12: 84.85%, Accuracy13: 81.29%, Accuracy14: 85.07%, Accuracy15: 86.41%, Accuracy16: 89.66%, Accuracy17: 82.79%, Accuracy18: 85.84%, Accuracy19: 90.78%, Accuracy20: 87.79%, Accuracy21: 85.36%, Accuracy22: 85.18%, Accuracy23: 90.77%, Accuracy24: 84.41%, Accuracy25: 96.13%, Accuracy26: 95.61%, \n",
      "Epoch [5/10], Training Loss: 0.0005, Validation Loss: 1.5866\n",
      "Training Accuracy : \n",
      "Accuracy1: 74.18%, Accuracy2: 90.63%, Accuracy3: 94.42%, Accuracy4: 67.69%, Accuracy5: 82.68%, Accuracy6: 69.37%, Accuracy7: 81.74%, Accuracy8: 88.16%, Accuracy9: 77.97%, Accuracy10: 84.18%, Accuracy11: 92.71%, Accuracy12: 84.03%, Accuracy13: 83.13%, Accuracy14: 85.17%, Accuracy15: 85.97%, Accuracy16: 90.21%, Accuracy17: 82.82%, Accuracy18: 88.51%, Accuracy19: 92.23%, Accuracy20: 90.28%, Accuracy21: 85.75%, Accuracy22: 86.12%, Accuracy23: 91.77%, Accuracy24: 91.12%, Accuracy25: 97.10%, Accuracy26: 96.73%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 67.33%, Accuracy2: 79.56%, Accuracy3: 92.30%, Accuracy4: 63.20%, Accuracy5: 80.72%, Accuracy6: 64.45%, Accuracy7: 79.16%, Accuracy8: 87.52%, Accuracy9: 75.43%, Accuracy10: 83.36%, Accuracy11: 92.85%, Accuracy12: 84.44%, Accuracy13: 81.77%, Accuracy14: 85.28%, Accuracy15: 85.52%, Accuracy16: 89.66%, Accuracy17: 82.96%, Accuracy18: 86.78%, Accuracy19: 91.27%, Accuracy20: 89.21%, Accuracy21: 82.92%, Accuracy22: 85.37%, Accuracy23: 90.64%, Accuracy24: 90.05%, Accuracy25: 94.18%, Accuracy26: 93.11%, \n",
      "Epoch [6/10], Training Loss: 0.0004, Validation Loss: 1.5234\n",
      "Training Accuracy : \n",
      "Accuracy1: 74.90%, Accuracy2: 91.49%, Accuracy3: 94.69%, Accuracy4: 67.84%, Accuracy5: 83.14%, Accuracy6: 69.54%, Accuracy7: 81.71%, Accuracy8: 88.30%, Accuracy9: 78.01%, Accuracy10: 84.27%, Accuracy11: 92.96%, Accuracy12: 84.13%, Accuracy13: 82.93%, Accuracy14: 85.24%, Accuracy15: 86.25%, Accuracy16: 90.41%, Accuracy17: 82.98%, Accuracy18: 88.52%, Accuracy19: 92.35%, Accuracy20: 90.19%, Accuracy21: 86.02%, Accuracy22: 86.15%, Accuracy23: 91.96%, Accuracy24: 91.20%, Accuracy25: 97.12%, Accuracy26: 96.97%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 67.69%, Accuracy2: 78.69%, Accuracy3: 91.34%, Accuracy4: 64.83%, Accuracy5: 80.44%, Accuracy6: 60.91%, Accuracy7: 74.22%, Accuracy8: 87.21%, Accuracy9: 77.32%, Accuracy10: 84.00%, Accuracy11: 92.78%, Accuracy12: 84.08%, Accuracy13: 81.76%, Accuracy14: 85.06%, Accuracy15: 86.44%, Accuracy16: 89.86%, Accuracy17: 82.83%, Accuracy18: 88.04%, Accuracy19: 91.84%, Accuracy20: 89.24%, Accuracy21: 85.22%, Accuracy22: 84.45%, Accuracy23: 91.00%, Accuracy24: 89.27%, Accuracy25: 96.48%, Accuracy26: 96.22%, \n",
      "Epoch [7/10], Training Loss: 0.0004, Validation Loss: 1.5104\n",
      "Training Accuracy : \n",
      "Accuracy1: 75.28%, Accuracy2: 92.04%, Accuracy3: 94.76%, Accuracy4: 68.07%, Accuracy5: 82.77%, Accuracy6: 69.46%, Accuracy7: 81.63%, Accuracy8: 88.15%, Accuracy9: 77.84%, Accuracy10: 84.09%, Accuracy11: 92.87%, Accuracy12: 84.50%, Accuracy13: 82.96%, Accuracy14: 85.24%, Accuracy15: 86.42%, Accuracy16: 90.40%, Accuracy17: 83.17%, Accuracy18: 88.64%, Accuracy19: 92.25%, Accuracy20: 90.36%, Accuracy21: 86.00%, Accuracy22: 86.17%, Accuracy23: 91.98%, Accuracy24: 91.07%, Accuracy25: 97.28%, Accuracy26: 97.00%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 69.50%, Accuracy2: 80.23%, Accuracy3: 93.08%, Accuracy4: 60.50%, Accuracy5: 79.24%, Accuracy6: 65.22%, Accuracy7: 80.26%, Accuracy8: 87.02%, Accuracy9: 72.02%, Accuracy10: 84.33%, Accuracy11: 92.28%, Accuracy12: 81.91%, Accuracy13: 78.29%, Accuracy14: 84.46%, Accuracy15: 86.85%, Accuracy16: 89.15%, Accuracy17: 82.82%, Accuracy18: 83.47%, Accuracy19: 91.26%, Accuracy20: 89.88%, Accuracy21: 85.22%, Accuracy22: 85.87%, Accuracy23: 91.37%, Accuracy24: 90.75%, Accuracy25: 95.41%, Accuracy26: 95.59%, \n",
      "Epoch [8/10], Training Loss: 0.0004, Validation Loss: 1.4481\n",
      "Training Accuracy : \n",
      "Accuracy1: 75.31%, Accuracy2: 92.73%, Accuracy3: 94.75%, Accuracy4: 67.94%, Accuracy5: 83.01%, Accuracy6: 70.21%, Accuracy7: 81.91%, Accuracy8: 88.34%, Accuracy9: 77.81%, Accuracy10: 84.46%, Accuracy11: 92.92%, Accuracy12: 84.22%, Accuracy13: 82.98%, Accuracy14: 85.30%, Accuracy15: 86.29%, Accuracy16: 90.55%, Accuracy17: 83.04%, Accuracy18: 88.63%, Accuracy19: 92.47%, Accuracy20: 90.27%, Accuracy21: 85.97%, Accuracy22: 86.35%, Accuracy23: 91.88%, Accuracy24: 91.18%, Accuracy25: 97.31%, Accuracy26: 96.99%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 70.17%, Accuracy2: 81.02%, Accuracy3: 93.68%, Accuracy4: 62.92%, Accuracy5: 79.22%, Accuracy6: 63.59%, Accuracy7: 79.35%, Accuracy8: 87.06%, Accuracy9: 76.83%, Accuracy10: 81.96%, Accuracy11: 89.82%, Accuracy12: 81.07%, Accuracy13: 80.06%, Accuracy14: 86.12%, Accuracy15: 86.23%, Accuracy16: 88.30%, Accuracy17: 78.30%, Accuracy18: 87.83%, Accuracy19: 91.50%, Accuracy20: 88.06%, Accuracy21: 83.68%, Accuracy22: 84.18%, Accuracy23: 90.53%, Accuracy24: 86.41%, Accuracy25: 96.14%, Accuracy26: 96.28%, \n",
      "Epoch [9/10], Training Loss: 0.0004, Validation Loss: 1.4335\n",
      "Training Accuracy : \n",
      "Accuracy1: 75.89%, Accuracy2: 93.16%, Accuracy3: 94.83%, Accuracy4: 68.01%, Accuracy5: 82.92%, Accuracy6: 70.59%, Accuracy7: 81.82%, Accuracy8: 88.52%, Accuracy9: 78.14%, Accuracy10: 84.40%, Accuracy11: 92.95%, Accuracy12: 84.19%, Accuracy13: 82.86%, Accuracy14: 85.29%, Accuracy15: 86.35%, Accuracy16: 90.69%, Accuracy17: 82.81%, Accuracy18: 88.69%, Accuracy19: 92.46%, Accuracy20: 90.41%, Accuracy21: 86.34%, Accuracy22: 86.30%, Accuracy23: 92.07%, Accuracy24: 91.25%, Accuracy25: 97.50%, Accuracy26: 97.26%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 68.87%, Accuracy2: 79.42%, Accuracy3: 93.73%, Accuracy4: 57.02%, Accuracy5: 81.40%, Accuracy6: 65.90%, Accuracy7: 79.46%, Accuracy8: 87.49%, Accuracy9: 77.26%, Accuracy10: 84.60%, Accuracy11: 90.96%, Accuracy12: 85.12%, Accuracy13: 83.81%, Accuracy14: 84.13%, Accuracy15: 86.95%, Accuracy16: 88.81%, Accuracy17: 82.68%, Accuracy18: 86.04%, Accuracy19: 89.52%, Accuracy20: 89.84%, Accuracy21: 83.37%, Accuracy22: 85.85%, Accuracy23: 90.72%, Accuracy24: 90.52%, Accuracy25: 96.59%, Accuracy26: 95.34%, \n",
      "Epoch [10/10], Training Loss: 0.0004, Validation Loss: 1.3835\n",
      "Training Accuracy : \n",
      "Accuracy1: 76.32%, Accuracy2: 93.63%, Accuracy3: 94.92%, Accuracy4: 68.10%, Accuracy5: 83.34%, Accuracy6: 70.74%, Accuracy7: 81.97%, Accuracy8: 88.57%, Accuracy9: 77.60%, Accuracy10: 84.41%, Accuracy11: 93.05%, Accuracy12: 84.39%, Accuracy13: 82.84%, Accuracy14: 85.39%, Accuracy15: 86.49%, Accuracy16: 90.78%, Accuracy17: 82.93%, Accuracy18: 88.62%, Accuracy19: 92.53%, Accuracy20: 90.57%, Accuracy21: 86.10%, Accuracy22: 86.33%, Accuracy23: 91.91%, Accuracy24: 91.51%, Accuracy25: 97.50%, Accuracy26: 97.29%, \n",
      "Validation Accuracy : \n",
      "Accuracy1: 69.67%, Accuracy2: 80.75%, Accuracy3: 93.48%, Accuracy4: 63.23%, Accuracy5: 80.92%, Accuracy6: 66.16%, Accuracy7: 79.84%, Accuracy8: 85.82%, Accuracy9: 75.63%, Accuracy10: 83.63%, Accuracy11: 92.94%, Accuracy12: 82.88%, Accuracy13: 82.35%, Accuracy14: 84.39%, Accuracy15: 86.17%, Accuracy16: 89.82%, Accuracy17: 82.49%, Accuracy18: 87.76%, Accuracy19: 92.27%, Accuracy20: 88.96%, Accuracy21: 82.14%, Accuracy22: 80.70%, Accuracy23: 91.08%, Accuracy24: 89.89%, Accuracy25: 96.63%, Accuracy26: 96.48%, \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#다중 출력 모델\n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "\n",
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "\n",
    "validation_loss_list = []\n",
    "validation_accuracy_list = []\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_corrects = [0] * len(labels)\n",
    "    for images, *labels in train_data_loader:  # Use *labels to unpack the list of label tensors\n",
    "        images = images.to(device)\n",
    "        labels = [label.to(device) for label in labels]  # Move each label tensor to the device\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        integer_labels = [torch.argmax(label, dim=1) for label in labels]\n",
    "        \n",
    "        # Calculate loss for each output\n",
    "        losses = [criterion(output, label) for output, label in zip(outputs, integer_labels)]\n",
    "        \n",
    "        # Sum the losses\n",
    "        total_loss = sum(losses)\n",
    "        '''\n",
    "\n",
    "        loss_list.append(total_loss.item()/(len(loss_list)+1))\n",
    "        if len(epoch_list) == 0:\n",
    "            epoch_list.append(1/len(train_data_loader))\n",
    "        else:\n",
    "            epoch_list.append(epoch_list[-1] + 1/len(train_data_loader))\n",
    "            \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.plot(epoch_list,loss_list, label='Training Loss', color='blue')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xticks([i for i in range(num_epochs+1)])\n",
    "        plt.yticks([i for i in range(3)])\n",
    "        plt.ylim(0,3)\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "        '''\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        corrects = [torch.sum(torch.argmax(output, 1) == torch.argmax(label, 1)).item() for output, label in zip(outputs, labels)]\n",
    "        total_corrects = [total + correct for total, correct in zip(total_corrects, corrects)]\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data_loader.dataset)\n",
    "    train_accuracies = [total_correct / len(train_data_loader.dataset) for total_correct in total_corrects]\n",
    "\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    train_accuracy_list.append(train_accuracies)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_corrects = [0] * len(labels)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, *labels in validation_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = [label.to(device) for label in labels]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            integer_labels = [torch.argmax(label, dim=1) for label in labels]\n",
    "\n",
    "            # Calculate loss for each output\n",
    "            losses = [criterion(output, label) for output, label in zip(outputs, integer_labels)]\n",
    "\n",
    "            # Sum the losses\n",
    "            total_loss += sum(losses).item()\n",
    "\n",
    "            # Calculate total correct predictions for each output\n",
    "            corrects = [torch.sum(torch.argmax(output, 1) == torch.argmax(label, 1)).item() for output, label in zip(outputs, labels)]\n",
    "            total_corrects = [total + correct for total, correct in zip(total_corrects, corrects)]\n",
    "\n",
    "    # Calculate average loss and accuracy for each output\n",
    "    avg_validation_loss = total_loss / len(validation_data_loader.dataset)\n",
    "    validation_accuracies = [total_correct / len(validation_data_loader.dataset) for total_correct in total_corrects]\n",
    "\n",
    "    validation_loss_list.append(avg_validation_loss)\n",
    "    validation_accuracy_list.append(validation_accuracies)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_validation_loss:.4f}\")\n",
    "    print(\"Training Accuracy : \")\n",
    "    for i, accuracy in enumerate(train_accuracies):\n",
    "        print(f\"Accuracy{i + 1}: {accuracy * 100:.2f}%, \", end=\"\")\n",
    "    print()\n",
    "    print(\"Validation Accuracy : \")\n",
    "    for i, accuracy in enumerate(validation_accuracies):\n",
    "        print(f\"Accuracy{i + 1}: {accuracy * 100:.2f}%, \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'multi_output_.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 모델 불러오기\n",
    "\n",
    "label_classes = []\n",
    "data_ex = next(iter(validation_dataset))\n",
    "for i in range(1,len(data_ex)):\n",
    "    labelSize = data_ex[i].size()\n",
    "    label_classes.append(labelSize.numel())\n",
    "print(label_classes)\n",
    "\n",
    "model = MultiOutputModel(label_classes)\n",
    "\n",
    "loaded_model.load_state_dict(torch.load('multi_output_model.pth'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
